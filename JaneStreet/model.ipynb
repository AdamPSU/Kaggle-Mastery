{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38f65e7-4001-4c56-83bf-9726f086dea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import warnings; warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from gc import collect\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e5640f-f821-469a-a14a-f0f87499f865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "\n",
    "TARGET = \"responder_6\"\n",
    "WEIGHT = 'weight'\n",
    "FEATURES = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e937768-db24-4800-b981-58e2448492c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_path): \n",
    "    id_col = pl.int_range(pl.len(), dtype=pl.UInt32).alias(\"id\") # Generate an id column\n",
    "    all_cols = pl.all() # Select all columns\n",
    "\n",
    "    # Read the parquet file and select the specified columns\n",
    "    data = pl.scan_parquet(file_path).select(id_col, all_cols)\n",
    "    \n",
    "    all_col_names = data.collect_schema().names()\n",
    "    \n",
    "    # Cols to not look for when classifying train and target column names\n",
    "    cols_of_disinterest = (\"weight\", \"id\", \"date_id\", \"time_id\", \"partition_id\")\n",
    "    target_columns, selected_columns = [], []\n",
    "\n",
    "    # Factory for loop to classify train and target column names\n",
    "    for col in all_col_names: \n",
    "        if col.startswith(\"responder\"):\n",
    "            target_columns.append(col)\n",
    "\n",
    "        elif not col.startswith(cols_of_disinterest):\n",
    "            selected_columns.append(col)\n",
    "            \n",
    "    data = data.collect()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35da025-1019-4242-aef1-3bc56c84e0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): ../data/train.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'select'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/train.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m dates_to_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      6\u001b[0m num_test_dates \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Read the parquet file and select the specified columns\u001b[39;00m\n\u001b[1;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mscan_parquet(file_path)\u001b[38;5;241m.\u001b[39mselect(id_col, all_cols)\n\u001b[0;32m----> 8\u001b[0m all_col_names \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnames()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Cols to not look for when classifying train and target column names\u001b[39;00m\n\u001b[1;32m     11\u001b[0m cols_of_disinterest \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartition_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/polars/lazyframe/frame.py:2266\u001b[0m, in \u001b[0;36mLazyFrame.collect_schema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_schema\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Schema:\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2238\u001b[0m \u001b[38;5;124;03m    Resolve the schema of this LazyFrame.\u001b[39;00m\n\u001b[1;32m   2239\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Schema(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, check_dtypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory (os error 2): ../data/train.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'select'\n"
     ]
    }
   ],
   "source": [
    "if TRAINING: \n",
    "    file_path = \"Kaggle-Mastery/JaneStreet/data/train.parquet\"\n",
    "\n",
    "    raw_data = load_data(file_path)\n",
    "\n",
    "    dates_to_skip = 500 # Too many missing stocks in the early days\n",
    "    num_test_dates = 100\n",
    "\n",
    "    # Filter the DataFrame to include only dates greater than or equal to dates_to_skip\n",
    "    raw_data = raw_data.filter(pl.col('date_id') >= dates_to_skip)\n",
    "\n",
    "    # Get unique dates from the DataFrame\n",
    "    dates = raw_data['date_id'].unique()\n",
    "\n",
    "    # Define validation dates as the last `num_test_dates` dates\n",
    "    test_dates = dates[-num_test_dates:]\n",
    "\n",
    "    # Define training dates as all dates except the last `num_test_dates` dates\n",
    "    train_dates = dates[:-num_test_dates]\n",
    "\n",
    "    raw_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038dad21-0e63-4a92-bd43-ba2cc7cb3057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare validation data for training\n",
    "if TRAINING: \n",
    "    test_data = raw_data.filter(pl.col('date_id').is_in(test_dates))\n",
    "\n",
    "    X_test = test_data[FEATURES]\n",
    "    y_test = test_data[TARGET]\n",
    "    w_test = test_data[WEIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a27b4b50-5c8d-48cc-a0bb-106dcfb51773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = 'Kaggle-Mastery/JaneStreet/models/xgb_01/'\n",
    "models = []\n",
    "\n",
    "def train(model, model_name=None):\n",
    "    # Not training, load `model_name` instead\n",
    "    if not TRAINING: \n",
    "        models.append(joblib.load(f'{model_path}/{model_name}_{i}.model'))\n",
    "        \n",
    "        return \n",
    "    \n",
    "    # Select dates for training based on the fold number\n",
    "    selected_dates = [date for ii, date in enumerate(train_dates) if ii % N_FOLDS != i]\n",
    "    \n",
    "    train_data = raw_data.filter(pl.col('date_id').is_in(train_dates))\n",
    "    \n",
    "    X_train = train_data[FEATURES]\n",
    "    y_train = train_data[TARGET]\n",
    "    w_train = train_data[WEIGHT]\n",
    "        \n",
    "    # Train XGBoost model with verbose logging\n",
    "    model.fit(X_train, y_train, sample_weight=w_train, \n",
    "              eval_set=[(X_test, y_test)], \n",
    "              sample_weight_eval_set=[w_test], \n",
    "              verbose=50)\n",
    "    \n",
    "    # Append the trained model to the list\n",
    "    models.append(model)\n",
    "    \n",
    "    del X_train, y_train, w_train\n",
    "    collect()\n",
    "\n",
    "    # Save the trained model to a file\n",
    "    joblib.dump(model, f'models/{model_name}_{i}.model')\n",
    "    \n",
    "    return \n",
    "\n",
    "# Custom R2 metric for XGBoost\n",
    "def r2_xgb(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "    \n",
    "    return -r2 # Must be negative for early stopping to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d21b3f-e418-49a9-a0c9-08b7d3205a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-r2_xgb:-0.00073\n",
      "[50]\tvalidation_0-r2_xgb:-0.00603\n",
      "[100]\tvalidation_0-r2_xgb:-0.00651\n",
      "[150]\tvalidation_0-r2_xgb:-0.00667\n",
      "[200]\tvalidation_0-r2_xgb:-0.00638\n",
      "[249]\tvalidation_0-r2_xgb:-0.00583\n",
      "[0]\tvalidation_0-r2_xgb:-0.00073\n",
      "[50]\tvalidation_0-r2_xgb:-0.00603\n",
      "[100]\tvalidation_0-r2_xgb:-0.00651\n",
      "[150]\tvalidation_0-r2_xgb:-0.00667\n",
      "[200]\tvalidation_0-r2_xgb:-0.00638\n",
      "[250]\tvalidation_0-r2_xgb:-0.00582\n",
      "[0]\tvalidation_0-r2_xgb:-0.00073\n",
      "[50]\tvalidation_0-r2_xgb:-0.00603\n",
      "[100]\tvalidation_0-r2_xgb:-0.00651\n",
      "[150]\tvalidation_0-r2_xgb:-0.00667\n",
      "[200]\tvalidation_0-r2_xgb:-0.00638\n",
      "[250]\tvalidation_0-r2_xgb:-0.00582\n",
      "[0]\tvalidation_0-r2_xgb:-0.00073\n",
      "[50]\tvalidation_0-r2_xgb:-0.00603\n",
      "[100]\tvalidation_0-r2_xgb:-0.00651\n",
      "[150]\tvalidation_0-r2_xgb:-0.00667\n",
      "[200]\tvalidation_0-r2_xgb:-0.00638\n",
      "[249]\tvalidation_0-r2_xgb:-0.00583\n",
      "[0]\tvalidation_0-r2_xgb:-0.00073\n",
      "[50]\tvalidation_0-r2_xgb:-0.00603\n",
      "[100]\tvalidation_0-r2_xgb:-0.00651\n",
      "[150]\tvalidation_0-r2_xgb:-0.00667\n",
      "[200]\tvalidation_0-r2_xgb:-0.00638\n",
      "[250]\tvalidation_0-r2_xgb:-0.00582\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {'n_estimators': 2000,\n",
    "              'learning_rate': 0.03, \n",
    "              'max_depth': 6, \n",
    "              'tree_method': 'hist', \n",
    "              'objective': 'reg:squarederror',\n",
    "              'early_stopping_rounds': 30,\n",
    "              'eval_metric': r2_xgb,\n",
    "              'disable_default_eval_metric': True,\n",
    "              'device': 'cuda'}\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    train(xgb)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
